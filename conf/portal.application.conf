# Copyright 2014 Brandon Arp
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ~~~~~

include "cluster.conf"

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
play.http.secret.key = "j;CsRfxs6n[6fA>Bb2^4@YVNdNw0omx=2L;=jqvGqfCuouukvF/KSrLJI?e:6xi4"

# The application languages
# ~~~~~
play.i18n.langs = ["en"]

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
play.modules.disabled += "play.core.ObjectMapperModule"
play.modules.enabled += "global.MainModule"
play.http.actionCreator = "global.ActionCreator"
play.http.errorHandler = "global.ErrorHandler"
play.http.filters = "global.Filters"
play.http.session.httpOnly = false
play.filters.cors.pathPrefixes = ["/v1/", "/api/v1/"]
play.filters.headers.contentSecurityPolicy = "script-src 'self' 'unsafe-eval'"

# Http Server
# ~~~~~
play.server.http.port = 8080

# Router
# ~~~~~
play.http.router = portal.Routes

# Health Provider
# ~~~~~
http.healthProvider.type = "com.arpnetworking.metrics.portal.health.DefaultHealthProvider"

# Features
# ~~~~~
portal.features {
  # Telemetry
  telemetry.enabled = true

  # Proxy
  proxy.enabled = true

  # Live Logging
  liveLogging.enabled = false

  # Host Registry
  hostRegistry.enabled = true

  # Expressions
  expressions.enabled = false

  # Alerts
  alerts.enabled = true
}

# Metrics
# ~~~~~
metrics {
  cluster = "metrics_portal"
  service = "metrics_portal"
  uri = "http://localhost:7090"
  jvm.interval = "500 millis"
}

# Database
# ~~~~~
db {

  # Switch between H2 and local Postgres

  # Option 1: H2
  default.username = "sa"
  default.password = "secret"
  default.url = "jdbc:h2:./target/h2/metrics_portal;AUTO_SERVER=TRUE;AUTO_SERVER_PORT=9091;MODE=PostgreSQL;INIT=create schema if not exists portal;DB_CLOSE_DELAY=-1"
  default.driver = "org.h2.Driver"
  metrics_portal_ddl.username = "sa"
  metrics_portal_ddl.password = "secret"
  metrics_portal_ddl.url = "jdbc:h2:./target/h2/metrics_portal;AUTO_SERVER=TRUE;AUTO_SERVER_PORT=9091;MODE=PostgreSQL;INIT=create schema if not exists portal;DB_CLOSE_DELAY=-1"
  metrics_portal_ddl.driver = "org.h2.Driver"
  metrics_portal_ddl.migration.locations = ["common", "h2"]

  # Option 2: Postgresql
  #default.username = "metrics_app"
  #default.password = "metrics_app_password"
  #default.url = "jdbc:postgresql://localhost:5432/metrics?currentSchema=portal"
  #default.driver = "org.postgresql.Driver"
  #metrics_portal_ddl.username = "metrics_dba"
  #metrics_portal_ddl.password = "metrics_dba_password"
  #metrics_portal_ddl.url = "jdbc:postgresql://localhost:5432/metrics?currentSchema=portal"
  #metrics_portal_ddl.driver = "org.postgresql.Driver"
  #metrics_portal_ddl.migration.locations = ["common", "postgresql"]

  # Common:
  default.hikaricp.poolName = "metrics_portal_dml"
  default.hikaricp.maximumPoolSize = 5
  metrics_portal_ddl.initOnMigrate = false
  metrics_portal_ddl.validateOnMigrate = true
  metrics_portal_ddl.encoding = "UTF-8"
  metrics_portal_ddl.migration.auto = true
  metrics_portal_ddl.migration.schemas = ["portal"]
  metrics_portal_ddl.hikaricp.poolName = "metrics_portal_ddl"
  metrics_portal_ddl.hikaricp.maximumPoolSize = 2
}

# Evolutions & Ebean
play.evolutions.enabled = false
play.modules.enabled += "org.flywaydb.play.PlayModule"
ebeanconfig.datasource.default = "default"
ebean.default = ["models.ebean.*"]

## EXAMPLE DATABASE CREATION ##
#
# First, connect to the admin database (e.g. postgres) with the root user (e.g. postgres):
#
#CREATE EXTENSION IF NOT EXISTS btree_gin;
#CREATE ROLE metrics_app LOGIN
#ALTER ROLE metrics_app WITH PASSWORD 'metrics_app_password';
#ALTER ROLE metrics_app WITH CONNECTION LIMIT 100;
#NOSUPERUSER INHERIT NOCREATEDB NOCREATEROLE NOREPLICATION CONNECTION LIMIT 10;
#CREATE ROLE metrics_dba LOGIN
#ALTER ROLE metrics_dba WITH PASSWORD 'metrics_dba_password';
#ALTER ROLE metrics_dba WITH CONNECTION LIMIT 100;
#NOSUPERUSER INHERIT NOCREATEDB NOCREATEROLE NOREPLICATION CONNECTION LIMIT 6;
#CREATE DATABASE metrics WITH OWNER = metrics_dba ENCODING = 'UTF8';
#GRANT CONNECT ON DATABASE metrics TO metrics_dba;
#GRANT CONNECT ON DATABASE metrics TO metrics_app;
#
# Next, connect to the metrics database with metrics_dba user:
#
#CREATE SCHEMA portal AUTHORIZATION metrics_dba;
#GRANT ALL ON SCHEMA portal TO metrics_dba;
#GRANT USAGE ON SCHEMA portal TO metrics_app;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON TABLES TO metrics_dba;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON SEQUENCES TO metrics_dba;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON FUNCTIONS TO metrics_dba;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON TYPES TO metrics_dba;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON TABLES TO metrics_app;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON SEQUENCES TO metrics_app;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON FUNCTIONS TO metrics_app;
#ALTER DEFAULT PRIVILEGES IN SCHEMA portal GRANT ALL ON TYPES TO metrics_app;

# Cassandra alert repository

play.modules.enabled += "global.PillarModule"
#cassandra.db.default {
#  clusterName = "Metrics"
#  hosts = ["localhost"]
#  keyspace = "portal"
## See http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeReplication_c.html
## for replication settings
#  replication {
#    class = "SimpleStrategy"
#    replication_factor = 1
#  }
#}

# Elastic search
# ~~~~~
elasticSearch {
  cluster.name = "Testing"
  node.local = true
  node.data = true
  path.logs = logs
  path.data = data

  discovery.zen.ping.unicast.hosts = ""
  discovery.zen.minimum_master_nodes = 1

  # Hosts index
  # ~~~~~
  index.hosts {
    shards = 1
    replicas = 0
    refresh = "1s"
  }
}

# Host repository
# ~~~~~
hostRepository.type = com.arpnetworking.metrics.portal.hosts.impl.DatabaseHostRepository
hostRepository.hostQueryGenerator.type = "com.arpnetworking.metrics.portal.hosts.impl.DatabaseHostRepository$GenericQueryGenerator"

# Host provider
# ~~~~~
hostProvider {
  type = com.arpnetworking.metrics.portal.hosts.impl.NoHostProvider
  initialDelay = 5 seconds
  interval = 5 seconds
}

# Alerts
# ~~~~~
alertRepository {
  type = com.arpnetworking.metrics.portal.alerts.impl.DatabaseAlertRepository
  alertQueryGenerator.type = "com.arpnetworking.metrics.portal.alerts.impl.DatabaseAlertRepository$GenericQueryGenerator"
}

# Expressions
# ~~~~~
expressionRepository {
  type = com.arpnetworking.metrics.portal.expressions.impl.NoExpressionRepository
  expressionQueryGenerator.type = "com.arpnetworking.metrics.portal.expressions.impl.DatabaseExpressionRepository$GenericQueryGenerator"
}

# Notifications
# ~~~~~
notificationRepository {
  type = com.arpnetworking.metrics.portal.notifications.impl.DatabaseNotificationRepository
  notificationQueryGenerator.type = "com.arpnetworking.metrics.portal.notifications.impl.DatabaseNotificationRepository$GenericQueryGenerator"
}

# KairosDB proxying
kairosdb.uri = "http://localhost:8000"

# Akka
# ~~~~~
akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  actor {
    debug {
      unhandled = on
    }
  }
  http {
    client {
      parsing.max-content-length = 104857600
      idle-timeout = 600s
    }
  }
}
play.server.akka.requestTimeout = 600s
play.server.http.idleTimeout = 600s
